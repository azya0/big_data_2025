# Лабораторная работа 10 (Связана с презой 27)

![alt text](10.png)

## Текстовое описание

1. Сформулируйте существенные признаки задач кластеризации и классификации. В чем их сходство и различие?
2. Классифицируйте методы классификации. Какие из методов классификации можно отнести к статистическим?
3. Сгенерируйте 3 выборки по 200 элементов из двумерных нормальных распределений с центрами в точках (3,3), (9,2), (9,6) и диагональными ковариационными матрицами с элементами (1.5,1.5), (1,1), (1,1). Решить задачу кластеризации методом к средних. Нарисуйте результирующие графики. Сделайте выводы.
4. Решить задачу классификации методом к ближайших соседей и наивным байесовским методом, используя решение предыдущей задачи по приведенным образцам в лекции. Построить таблицу результатов.

### Существенные признаки классификации:

- Наличие размеченных примеров: для обучения есть объекты с известными метками (классами).
- Цель - построить правило (модель), которое по признакам предсказывает метку класса для новых объектов.
- Подразумевается обучение с учителем (supervised learning).
- Оценка по метрикам качества классификации: accuracy, precision, recall, F1, ROC/AUC и т.д.
- Модель может быть как детерминистской (дерево), так и вероятностной (логистическая регрессия, наивный байес).

### Существенные признаки кластеризации:

- Данные, как правило, неразмечены — нет «истинных» меток.
- Цель - найти внутреннюю структуру: группы (кластеры) объектов, похожих друг на друга.
- Подразумевается обучение без учителя (unsupervised learning).
- Оценка качества чаще внутреняя (silhouette, Davies–Bouldin), либо внешняя (ARI, NMI) если есть эталонные метки.
- Методы могут быть разделяющие (k-means), иерархические, плотностные (DBSCAN), моделирующие (GMM) и т.д.

### Сходство между кластеризацией и классификацией
- Задача группировки объектов: Оба метода в итоге относят объекты к определенным категориям (группам).
- Работа с признаковым описанием: Входными данными в обоих случаях являются объекты, описанные набором признаков (features).
- Использование мер близости: Многие алгоритмы (особенно в классификации, как k-NN, и в кластеризации) так или иначе опираются на вычисление схожести или расстояния между объектами.

## Классификация методов классификации

1. По типу модели (способу формирования решающего правила)

- Геометрические (Разделяющие поверхности):
    - Идея: Находят границу (гиперповерхность) в пространстве признаков, разделяющую классы.
    - Примеры: Метод опорных векторов (SVM), линейная и логистическая регрессия (геометрическая интерпретация),
      перцептрон.
- Вероятностные (Статистические):
    - Идея: Моделируют вероятностное распределение данных и относят объект к классу с максимальной апостериорной
      вероятностью (теорема Байеса).
    - Примеры: Наивный байесовский классификатор, линейный и квадратичный дискриминантный анализ (LDA/QDA).
- Логические (Правила/Деревья):
    - Идея: Строят иерархическую структуру решающих правил вида «ЕСЛИ [условие на признаки], ТО [класс]».
    - Примеры: Деревья решений (CART, C4.5, C5.0), алгоритмы на основе правил (RIPPER).
- Методы на основе сходства (Ядерные/Инстансные):
    - Идея: Класс объекта определяется классами наиболее похожих на него объектов из обучающей выборки.
    - Примеры: Метод k ближайших соседей (k-NN), методы с применением ядерных функций (ядро в SVM).
- Ансамблевые (Композиционные):
    - Идея: Объединение прогнозов нескольких базовых классификаторов для повышения точности и устойчивости.
    - Примеры: Случайный лес (Random Forest), градиентный бустинг (Gradient Boosting, XGBoost, LightGBM, CatBoost),
      бэггинг (Bagging).
- Нейросетевые:
    - Идея: Использование искусственных нейронных сетей для обучения сложных нелинейных разделяющих функций.
    - Примеры: Многослойный перцептрон (MLP), сверточные нейронные сети (CNN - для изображений), рекуррентные нейронные
      сети (RNN - для последовательностей), трансформеры.

2. По характеру разделяющей границы

- Линейные методы: Строят линейную границу решения (гиперплоскость).
    - Примеры: Линейная регрессия (для классификации), логистическая регрессия, линейный SVM, LDA.
- Нелинейные методы: Строят сложные нелинейные границы.
    - Примеры: Деревья решений, нелинейный SVM (с ядрами), нейронные сети, случайный лес.

3. По способу обучения

- Прямые (дискриминантные) методы: Напрямую моделируют зависимость метки класса y от признаков X (оценивают P(y|X) или
  находят функцию f(X)=y).
    - Примеры: Большинство методов — логистическая регрессия, SVM, деревья, нейронные сети.
- Генеративные методы: Сначала моделируют распределение признаков для каждого класса P(X|y), а затем используют теорему
  Байеса для предсказания. Они «порождают» модель данных каждого класса.
    - Примеры: Наивный Байес, LDA/QDA.